{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "cmapRdYlGn = cm.get_cmap('RdYlGn')\n",
    "cmap = cm.coolwarm\n",
    "\n",
    "\n",
    "challenge_data_folder = \"./challenge_data\"\n",
    "\n",
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdAllData_Categ = pd.read_csv(challenge_data_folder+\"/train.csv\",keep_default_na=False) #Load training data NA filter FALSE to keep Not available data\n",
    "#Categ indicates that no label encoding has been performed, the categorical features are untouched\n",
    "columnTypesDict = pdAllData_Categ.dtypes.to_dict() #Infer part of the data schema\n",
    "columnNames = list(columnTypesDict.keys()) #List of all features name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first, the only information we have on the data set resides in the description file therefore we tried to match the data and the description file to make sure the data is clean and clear. One must first match features names and then specified values in the description file and in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Data cleaning\n",
    "def dataCleaning(description_file_Path):\n",
    "    \"\"\"Checks in the data for not mentioned values in the description file given.\"\"\"\n",
    "    pathDescr = (challenge_data_folder+description_file_Path)\n",
    "    txtDescripData = open(pathDescr).readlines()\n",
    "    columnValues={}\n",
    "    columnIndex=[]\n",
    "    nbHeader = 7\n",
    "    NoHeaderTxtDescripData = txtDescripData[nbHeader:]\n",
    "    #Parse the description file to retrieve all the lines that corresponds to features    \n",
    "    for feature in columnNames:\n",
    "        if feature == 'Id': continue\n",
    "        for index,line in enumerate(NoHeaderTxtDescripData):\n",
    "            if feature in line: #Match features name and stated name in the description\n",
    "                if(line.split(feature)[0]=='' and line.split(feature)[1][0]==':' ): #Make sure exactly this feature in this line \n",
    "                    columnIndex.append((feature,index)) #We retrieved the line where the feature is described\n",
    "                pass\n",
    "\n",
    "    NoSpecificValfeatures = []\n",
    "    SpecificVal = {} #Will contain as keys column names and values the specified values in the description file\n",
    "    \n",
    "    #Scan the cocument to retrieve specfic values for each featurz\n",
    "    for index,featureLine in enumerate(columnIndex):\n",
    "\n",
    "        startLine= featureLine[1]+2  #RTF file format \n",
    "        if index == len(columnIndex)-1: endLine = len(NoHeaderTxtDescripData)-1\n",
    "        else: endLine = columnIndex[index+1][1]-1\n",
    "        values = []\n",
    "        if endLine <= startLine: #No specified value in the description file\n",
    "            NoSpecificValfeatures.append(featureLine[0])\n",
    "            continue\n",
    "        for i in range(startLine,endLine):\n",
    "            line = NoHeaderTxtDescripData[i] \n",
    "            values.append(line.split('\\t')[0].strip())\n",
    "        SpecificVal[featureLine[0]]=values #All values specified are retrieved and kept in this dictionary\n",
    "        \n",
    "    #Retrieve Bad values for the columns with specified values\n",
    "    dicBadValues = []\n",
    "    for factor in SpecificVal:\n",
    "\n",
    "        #For a factor search for any value not specified in the description file \n",
    "        #In terms of type of valu\n",
    "\n",
    "        typeCol = columnTypesDict[factor]\n",
    "        if typeCol == 'int64': valSpec = [int(i) for i in SpecificVal[factor]]\n",
    "        else: valSpec = SpecificVal[factor]\n",
    "        pdInter = pdAllData_Categ.apply(lambda x: x[factor] not in valSpec,axis=1)\n",
    "        potentialBadValues = pdAllData_Categ[pdAllData_Categ.apply(lambda x: x[factor] not in valSpec,axis=1)]\n",
    "        nbBadVal = len(potentialBadValues)\n",
    "\n",
    "        if nbBadVal>0:\n",
    "            badValues = [factor]\n",
    "            badValues.append(potentialBadValues.groupby(factor)[factor].count().to_dict())\n",
    "            dicBadValues.append(badValues)\n",
    "    #Retrieve Bad values for the columns with not specified values such as year of construction of \n",
    "\n",
    "    for feature in NoSpecificValfeatures:\n",
    "        dtypeColumn = columnTypesDict[feature]\n",
    "        if dtypeColumn=='float64': typeFun = float #infered type -> check if any values do not convert to this type\n",
    "        elif dtypeColumn=='int64': typeFun = int\n",
    "        else: typeFun=lambda x: True\n",
    "        pdInter = pdAllData_Categ[pdAllData_Categ.apply(lambda x: (x[feature]=='NA') and typeFun(x[feature]) ,axis=1)]\n",
    "        if len(pdInter)>0:\n",
    "            badValues=[feature]\n",
    "            badValues.append(pdInter.groupby(feature)[feature].count().to_dict())\n",
    "            dicBadValues.append(badValues)\n",
    "    return SpecificVal,NoSpecificValfeatures,dicBadValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['MSZoning', {'C (all)': 9}],\n",
       " ['Neighborhood', {'NAmes': 179}],\n",
       " ['BldgType', {'2fmCon': 28, 'Duplex': 41, 'Twnhs': 37}],\n",
       " ['Exterior2nd', {'Brk Cmn': 7, 'CmentBd': 49, 'Wd Shng': 29}],\n",
       " ['MasVnrType', {'NA': 6}],\n",
       " ['LotFrontage', {'NA': 210}],\n",
       " ['MasVnrArea', {'NA': 6}],\n",
       " ['GarageYrBlt', {'NA': 67}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_description_file_Path = \"/Bad_Description.rtf\"\n",
    "bad_specificVal,bad_noSpecificValfeatures,bad_dicBadValues = dataCleaning(bad_description_file_Path)\n",
    "bad_dicBadValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['MSZoning', {'C (all)': 9}],\n",
       " ['BldgType', {'Twnhs': 37}],\n",
       " ['MasVnrType', {'NA': 6}],\n",
       " ['LotFrontage', {'NA': 210}],\n",
       " ['MasVnrArea', {'NA': 6}],\n",
       " ['GarageYrBlt', {'NA': 67}]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_file_Path = \"/Data description.rtf\"\n",
    "SpecificVal,NoSpecificValfeatures,dicBadValues = dataCleaning(description_file_Path)\n",
    "print(NoSpecificValfeatures)\n",
    "dicBadValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When GarageYrBlt = NA, GarageType has values  {'NA'}\n",
      "When MasVnrArea = NA, MasVnrArea has values {'NA'}\n"
     ]
    }
   ],
   "source": [
    "print('When GarageYrBlt = NA, GarageType has values ',set(pdAllData_Categ[pdAllData_Categ.apply(lambda x: x.GarageYrBlt=='NA',axis=1)].GarageType.values))\n",
    "print('When MasVnrArea = NA, MasVnrArea has values',set(pdAllData_Categ[pdAllData_Categ.apply(lambda x: x.MasVnrArea=='NA',axis=1)].MasVnrType.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "First of all, some column names were wrong in the description file erors. By deducing form the data, we changed \n",
    "Bedroom to BedroomAbvGr and \n",
    "Kitchen to KitchenAbvGr.<br>\n",
    "\n",
    "THere are also errors in the values described.<br>\n",
    "Most errors with the initial description are easily corrected by slighlty modifying the description file but some are more complex and persist such as :<br>\n",
    "<li>C(all)</li>\n",
    "<li>Twnhs</li>\n",
    "They have no direct typo or writings and the other values pecified are included in the data.<br>\n",
    "Although those values are not explicit in the description file, they contain information so we decided to keep them eventhough no direct explainition of those category is available.<br>\n",
    "<br>\n",
    "Otherwise, the other NA values can contain information or be explained by other specified values. The two cases above state that when a property has no garage, the year the garage has been built is not available and no MasVnrType implies no MasVnrArea which is reasonable but important to take into account.<br>\n",
    "\n",
    "To conclude on this, no purely invalid values have been spotted in the data. Plus the NA values are hard to distinguish between not measured or simply absent of the good that's why we decieded to keep them.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "        &emsp; The data may be clean, it contains unconsistent datatype; strings, int and float. However, as mentioned before, we decided to use a tree model to solve this problem and tree models give exceeding results on datasets with numerical features. Therefore the first step of our pipeline is to transform string values to numbers which is encode categorical features.<br>\n",
    "        &emsp;Encoding categorical features consists in transforming discrete string values to numerical values . To do so, many techniques exists, we decided to study two approches: label encoding and one hot encoding.<br><br>\n",
    "        &emsp;Label encoding consists in simply associating to a string value of a categorical feature to a int. For exemple, the Alley featurn has possible values Grvl, Pave and NA. A possible encoding would replace all Grvl values by 0, Pave values by 1 and NA values by 2. The main disadvantage of this simple technique is that it introduces an order between the numerical values that can lead to misinterpretation by the model of this feature.\n",
    "        &emsp;The one-hot encoding tries to cope with this problem by intoducing dummy variable. In short, for each possible values, we introduce a new column which values will be 1 where the original features had this value in the frist place 0 otherwise. In the case of the alley, we would add three column Alley_Pave, Alley_Grvl and Alley_NA. If a house has no alley then we would have 0 in Alley_Pave and Alley_Grvl and a 1 in the Alley-NA column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeCategoricalFeatures(Data,labelEncode=True,oneHotEncode=False):\n",
    "    \"\"\"Encode the input dataframe Data using the label or one hot encoding technique \"\"\"\n",
    "    if labelEncode: #Label Encoding technique\n",
    "        lb_make = LabelEncoder()\n",
    "        for featureCol in Data:\n",
    "            if Data[featureCol].dtype=='object':\n",
    "                Data[featureCol] = lb_make.fit_transform(Data[featureCol])\n",
    "            else:\n",
    "                continue\n",
    "        return Data\n",
    "    elif oneHotEncode:\n",
    "        data_frames =[]\n",
    "        for featureCol in Data:\n",
    "            if Data[featureCol].dtype=='object' and featureCol in SpecificVal:\n",
    "                lb_style = LabelBinarizer()\n",
    "                lb_results = lb_style.fit_transform(Data[featureCol])\n",
    "                if(len(lb_style.classes_)==2):columnsNames = [featureCol] #Simple binarisation -> on column resulting for two classes \n",
    "                else: columnsNames = lb_style.classes_ #One columnfor each possible values \n",
    "\n",
    "                lb_results = pd.DataFrame(lb_results, columns=[featureCol+\"_\"+name for name in columnsNames])\n",
    "                data_frames.append(lb_results)\n",
    "            elif Data[featureCol].dtype=='object': #NA values in a number column\n",
    "                lb_make = LabelEncoder()\n",
    "                data_frames.append(pd.DataFrame(lb_make.fit_transform(Data[featureCol]),columns=[featureCol]))\n",
    "            else:\n",
    "                data_frames.append(Data[featureCol])\n",
    "        else: raise Exception('Choose an encoding technique labelEncode=True or oneHotEncode=True')\n",
    "        return pd.concat(data_frames,axis=1)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
